{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depreciated!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "# set matplotlib font to arial\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = mps\n"
     ]
    }
   ],
   "source": [
    " # if GPU is to be used\n",
    "DEVICE = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"device = {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mForagingEnv\u001b[39;00m(\u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mEnv):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, ser_lvl\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, grid_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, reward_block\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m180\u001b[39m, \u001b[38;5;241m180\u001b[39m, \u001b[38;5;241m184\u001b[39m, \u001b[38;5;241m184\u001b[39m), penalty_block\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m54\u001b[39m, \u001b[38;5;241m54\u001b[39m), initial_agent_pos \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m]):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(ForagingEnv, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "class ForagingEnv(gym.Env):\n",
    "    def __init__(self, ser_lvl=0.5, grid_size=200, reward_block=(180, 180, 184, 184), penalty_block=(40, 40, 54, 54), initial_agent_pos = [100, 100]):\n",
    "        super(ForagingEnv, self).__init__()\n",
    "\n",
    "        self.grid_size = grid_size # set the grid size\n",
    "        self.reward_block = reward_block # define a 4x4 reward block (x_start, y_start, x_end, y_end)\n",
    "        self.penalty_block = penalty_block # define as 4x4 penalty block (x_start, y_start, x_end, y_end)\n",
    "        self.state = np.zeros((self.grid_size, self.grid_size)) # initialize the state\n",
    "        self.agent_pos = initial_agent_pos # set the initial agent position\n",
    "        self.action_space = spaces.Discrete(4) # define the action space: 4 actions (up, down, left, right)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(self.grid_size, self.grid_size), dtype=np.float32) # define the observation space\n",
    "        self.serotonin_level = ser_lvl # set the initial serotonin level\n",
    "        self.serotonin_level_0 = ser_lvl # set the initial serotonin level -- helpful in reset\n",
    "        self.initial_agent_pos = initial_agent_pos # set the initial agent position -- helpful in reset\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment and agent's position\n",
    "        \"\"\"\n",
    "        self.state = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.agent_pos = self.initial_agent_pos\n",
    "        self.serotonin_level = self.serotonin_level_0\n",
    "        return self.state # return the state\n",
    "    \n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Take an action and update the environment.\n",
    "        1. [0] controls up/down\n",
    "        2. [1] controls left/right\n",
    "        '''\n",
    "        # update agent's position based on the action\n",
    "        # up\n",
    "        if action == 0:\n",
    "            self.agent_pos[0] = max(0, self.agent_pos[0] - 1)\n",
    "        # down\n",
    "        elif action == 1:\n",
    "            self.agent_pos[0] = min(self.grid_size - 1, self.agent_pos[0] + 1)\n",
    "\n",
    "        # left\n",
    "        elif action == 2:\n",
    "            self.agent_pos[1] = max(0, self.agent_pos[1] - 1)\n",
    "\n",
    "        # right\n",
    "        elif action == 3:\n",
    "            self.agent_pos[1] = min(self.grid_size - 1, self.agent_pos[1] + 1)\n",
    "\n",
    "        ## set done flag\n",
    "        done = False\n",
    "        reward = -1 # default reward\n",
    "\n",
    "        ## check if the agent is in the reward block\n",
    "        if self.reward_block[0] <= self.agent_pos[0] <= self.reward_block[2] and self.reward_block[1] <= self.agent_pos[1] <= self.reward_block[3]:\n",
    "            reward = 100\n",
    "            done = True\n",
    "\n",
    "        ## check if the agent is in the penalty block\n",
    "        if self.penalty_block[0] <= self.agent_pos[0] <= self.penalty_block[2] and self.penalty_block[1] <= self.agent_pos[1] <= self.penalty_block[3]:\n",
    "            reward = -100\n",
    "            done = True\n",
    "\n",
    "        ## TODO: update serotonin level if needed\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def render(self, mode = 'human'):\n",
    "        '''\n",
    "        Render the environment\n",
    "        '''\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_xlim(0, self.grid_size)\n",
    "        ax.set_ylim(0, self.grid_size)\n",
    "\n",
    "        # draw the reward block\n",
    "        reward_patch = mpatches.Rectangle((self.reward_block[0], self.reward_block[1]), self.reward_block[2] - self.reward_block[0], self.reward_block[3] - self.reward_block[1], color='green')\n",
    "        ax.add_patch(reward_patch)\n",
    "\n",
    "        # draw the penalty block\n",
    "        penalty_patch = mpatches.Rectangle((self.penalty_block[0], self.penalty_block[1]), self.penalty_block[2] - self.penalty_block[0], self.penalty_block[3] - self.penalty_block[1], color='red')\n",
    "        ax.add_patch(penalty_patch)\n",
    "\n",
    "        # draw the agent\n",
    "        ax.plot(self.agent_pos[1], self.agent_pos[0], 'ko', label = 'Agent')\n",
    "\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "\n",
    "        plt.gca().invert_yaxis()\n",
    "        # plt.axis('off')\n",
    "        plt.grid(True)\n",
    "        # plt.legend()\n",
    "\n",
    "\n",
    "        # sns.despine()\n",
    "\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "## test the environment\n",
    "\n",
    "env =  ForagingEnv()\n",
    "env.reset()\n",
    "# env.render()\n",
    "\n",
    "# # simulate 1 step\n",
    "# env.step(1)  # Move the agent down\n",
    "# env.render()  # Render the environment\n",
    "\n",
    "# env.step(3)  # Move the agent down\n",
    "# env.render()  # Render the environment\n",
    "\n",
    "steps_arr = np.random.randint(0, 4, 100)\n",
    "for step in steps_arr:\n",
    "    env.step(step)\n",
    "\n",
    "env.render()  # Render the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a deep Q-network\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "# Defining the agent\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dimension, action_dimension, device = DEVICE):\n",
    "        self.state_dimension = state_dimension\n",
    "        self.action_dimension = action_dimension\n",
    "        self.q_network = DQN(state_dimension, action_dimension).to(device)\n",
    "        self.target_network = DQN(state_dimension, action_dimension).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001, momentum=0.9)\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.device = device\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telluride24_llm_bifurcation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
